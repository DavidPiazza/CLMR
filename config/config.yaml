# distributed training
gpus: 0 # I recommend always assigning 1 GPU to 1 node
workers: 3
dataset_dir: "./data"
time_domain: 1 # train in the time domain. or time-frequency domain
supervised: 0 # train with supervised baseline

# train options
seed: 42
batch_size: 48
start_epoch: 0
epochs: 100
dataset: "magnatagatune"

# model options
resnet: "resnet18"
projection_dim: 64 # Projection dim. of SimCLR projector

# loss options
optimizer: "Adam" # or LARS (experimental)
learning_rate: 0.0003
weight_decay: 1.0e-6 # "optimized using LARS [...] and weight decay of 10âˆ’6"
temperature: 0.5 # see appendix B.7.: Optimal temperature under different batch sizes

# reload options
checkpoint_path: "" # set to the directory containing `checkpoint_##.tar`

# logistic regression options
linear_checkpoint_path: ""
logistic_batch_size: 256
logistic_epochs: 500

# audio transformations
audio_length: 59049
sample_rate: 16000
transforms_polarity: 0.8
transforms_noise: 0.01
transforms_gain: 0.3
transforms_filters: 0.8
transforms_delay: 0.3
transforms_pitch: 0.6
transforms_reverb: 0.6
